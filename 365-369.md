# **可变范围**
可变范围是TensorFlow的一项功能，可帮助我们执行以下操作：
确保我们有一些命名约定以便以后检索它们，例如，通过使用单词生成器或鉴别器开始它们，这将在网络培训期间帮助我们。 我们可以使用这个名字范围功能，
但此功能无法帮助我们实现第二个目的；
能够重用或重新训练相同的网络但具有不同的输入。 例如，我们将从发电机中抽取假图像，看看它有多好生成器用于复制原始生成器。 此外，鉴别器将具
有访问真实和虚假的图像，这将使我们很容易重用变量而不是在构建计算图时创建新变量.

以下语句将说明如何使用TensorFlow的可变范围功能：

```tf.variable_scope('scopeName',reuse=False)```

**https://www.tensorflow.org/programmers_guide/variable_scope#the_problem可以阅读有关使用可变范围功能的好处的更多信息**
# **Leaky ReLU**
我们提到过我们将使用与ReLU激活功能不同的版本，这被称为Leaky ReLU.传统版本的ReLU激活功能就是这样取输入值和零之间的最大值，通过其他方式截断负值
值为零.Leaky ReLU，这是我们将使用的版本，允许一些负值存在，因此名称Leaky ReLU.
有时，如果我们使用传统的ReLU激活功能，网络就会陷入流行的状态叫做垂死的状态，那是因为网络什么也没产生所有输出都为零。
使用Leaky ReLU的想法是通过允许一些负值, 以防止这种死亡状态要传递的值。
使生成器工作背后的整个想法是从鉴别器接受梯度值, 如果网络陷入垂死的情况下, 学习过程不会发生。

下图说明了传统的ReLU与其Leaky版本之间的区别：

Leaky ReLU激活函数没有在TensorFlow中实现, 因此我们需要自己来实现它。如果输入是正值，此激活函数的输出将是正的,如果输入为负值, 则为受控负值。我们将控制一个名为alpha的参数的负值, 这将通过允许一些负值引入网络的容差。

下面的公式表示我们正在执行的Leaky ReLU:

f(x)=max(a×x,x)
# 生成器
mnist图像在0和1之间进行归一化, 其中tjhnpje激活函数可以表现得最好。但在实践中发现，与其他函数相比，uboi激活函数能提供更好的性能。因此, 为了使用uboi激活函数, 我们需要将这些图像的像素值的范围重新缩放到-1 和1之间:

```def generator(gen_z,gen_out_dim,num_hiddern_units=128,reuse_vars=False,leaky_relu_alpha=0.01);```

构建网络生成器部分,函数参数:gen_z:发电机输入张量;gen_out_dim:发电机的输出形状;num_hiddern_units:神经元的数量/隐藏层中的单位：reuse_vars:tf.variable_scope中的重用变量；leaky_relu:Leaky ReLU参数;函数返回:sigmoid_out;logits_layer

```tf.variable_scope('generator',reuse=reuse_vars)  定义生成器隐藏层```

```hidden_layer_1=tf.layers.dense(qen_z,num_hiddern_units,activation=None)   将hidden_layer_1的输出送到leaky relu中```

```hidden_layer_1=tf.maximum(hidden_layer_1,leaky_relu_alpha*hidden_layer_1)    获取logits和tanh层的输出```

```logits_layer=tf.layers.dense(hidden_layer_1,gen_out_dim,activation=None)```

```tanh_output=tf.nn.tanh(logits_layer)```

```return tanh_output,lagits_layer```

现在我们已经准备好了发电机部分。让我们继续前进, 并定义的第二个组件的网络
 # 判别器
 接下来, 我们将构建生成对抗网络中的第二个主要组件,这就是判别器。判别器与生层器大同小异, 但不是使用uboi激活函数, 我们将使用tjhnpj激活
 功能;它将产生一个二进制输出, 将代表判断输入图像上的鉴别器:
 
 ```def discriminator(disc_input,num_hiddern_units=128,reuse_vars=False,leaky_relu_alpha=0.01)```
 
 构建网络判别器部分,函数参数:disc_input:判别器输入张量;num_hiddern_units:神经元的数量/隐藏层中的单位：reuse_vars:tf.variable_scope中的重用变量；leaky_relu_alpha:Leaky ReLU参数;函数返回:sigmoid_out;logits_layer
 
 ```tf.variable_scope('discriminator',reuse=reuse_vars)    定义判别器隐藏层```
 
 ```hidden_layer_1=tf.layers.dense(disc_input,num_hiddern_units,activation=None)   将hidden_layer_1的输出送到leaky relu中```
 
 ```hidden_layer_1=tf.maximum(hidden_layer_1,leaky_relu_alpha*hidden_layer_1)```
 
 ```logits_layer=tf.layers.dense(hidden_layer_1,1,activation=None)```
 
 ```sigmoid_out=tf.nn.sigmoid(logits_layer)```
 
 ```return sigmoid_out,logits_layer```
 
 # 建立GAN网络
 在定义构建生成器和判别器部件的主要功能后,实现把它们堆叠在一起并且定义模型丢失和优化器的功能。
 # 模型超参数
 我们可以通过更改以下一组超参数来微调GANs:
 ```input_img_size=784  生成器输入图像的大小将会以28×28平铺成784```
 ```gen_z_size=100  生成器潜在向量的大小```
 ```gen_hidden_size=128```
 ```disc_hidden_size=128  生成器和判别器隐藏层中的隐藏单元的数量```
 ```leaky_relu_alpha=0.01 控制leak功能的leaky relu  alpha参数``` 
 ```lable_smooth=0.1```
 标签平滑度
 # 定义生成器和判别器
 在定义了我们的体系结构的两个主要部分后, 将被用来生成假mnist 图像 (看起来与真实图像完全相同), 是时候构建网络使用的功能, 我们已经定义到目前为止。为了建立网络, 我们将按照以下步骤操作:
 1.定义模型的输入, 该模型将由两个变量组成。这些变量将是真实的图像中的一个, 这将被提供给鉴别器, 并第二个将是生成器用来复制原始图像的潜在空间。
 2.调用定义的生成器函数来构建网络的生成器部分。
 3.调用定义的判别器函数来构建网络的判别器部分，但我们将调用此函数两次。第一次调用将是真正的数据和第二次调用将是来自生成器的假数据。
 4.通过重用变量保持真实和假图像的权重不变:
 ```tf.reset_default_graph()```
 为生成器和判别器创建占位符
 ```real_discriminator_input,generator_input_z=inputs_placeholders(input_img_size,gen_z_size)```
 创建生成器网络
 ```gen_modle,gen_logits=generator(generator_input_z,input_img_size,gen_hidden_size,reuse_vars=False,leaky_relu_alpha=leaky_relu_alpha)```
 gen_modle是生成器的输出
 创建判别器网络
 ```disc_modle_real,disc_logits_real=discriminator(real_discriminator_input,disc_hidden_size,reuse_vars=False,leaky_relu_alpha=leaky_relu_alpha)```
 ```disc_modle_fake,disc_logits_fake=discriminator(gen_modle,disc_hidden_size,reuse_vars=True,leaky_relu_alpha=leaky_relu_alpha)```






