# **可变范围**

## 可变范围是TensorFlow的一项功能，可帮助我们执行以下操作：
### 确保我们有一些命名约定以便以后检索它们，例如，通过使用单词生成器或鉴别器开始它们，这将在网络培训期间帮助我们。 我们可以使用这个名字范围功能，
但此功能无法帮助我们实现第二个目的；
### 能够重用或重新训练相同的网络但具有不同的输入。 例如，我们将从发电机中抽取假图像，看看它有多好生成器用于复制原始生成器。 此外，鉴别器将具
有访问真实和虚假的图像，这将使我们很容易重用变量而不是在构建计算图时创建新变量.
## 以下语句将说明如何使用TensorFlow的可变范围功能：
tf.variable_scope('scopeName',reuse=False)
## 在https://www.tensorflow.org/programmers_guide/variable_scope#the_problem可以阅读有关使用可变范围功能的好处的更多信息
# **Leaky ReLU**
我们提到过我们将使用与ReLU激活功能不同的版本，这被称为Leaky ReLU.传统版本的ReLU激活功能就是这样取输入值和零之间的最大值，通过其他方式截断负值
值为零.Leaky ReLU，这是我们将使用的版本，允许一些负值存在，因此名称Leaky ReLU.
有时，如果我们使用传统的ReLU激活功能，网络就会陷入流行的状态叫做垂死的状态，那是因为网络什么也没产生所有输出都为零。
使用Leaky ReLU的想法是通过允许一些负值, 以防止这种死亡状态要传递的值。
使生成器工作背后的整个想法是从鉴别器接受梯度值, 如果网络陷入垂死的情况下, 学习过程不会发生。
下图说明了传统的ReLU与其Leaky版本之间的区别：
